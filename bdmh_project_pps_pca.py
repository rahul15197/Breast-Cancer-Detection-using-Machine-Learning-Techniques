# -*- coding: utf-8 -*-
"""BDMH_Project_PPS_PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U0AmqNC_L6vM924ogM0pEIxo6cS3io0F

# Prediction of Breast Cancer using Supervised Machine Learning Techniques

The python program classifies whether the patient is suffering from Breast cancer or not (malignant or benign tumour) based on the training done on the training dataset.
"""

pip install ppscore

# importing all the necessary libraries
import numpy as np
import pandas as pd
from decimal import Decimal
from sklearn.decomposition import PCA
import ppscore as pps
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as mpatches
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report
from keras import Sequential
from keras.layers import Dense, Dropout
from fastai import *
from fastai.tabular import * 
import warnings
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from statistics import mean
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.utils import shuffle
# ignore warnings faced due to usage of old version of tensorflow
warnings.simplefilter('ignore')

"""Breast Cancer Wisconsin (Diagnostic) Dataset (WDBC) is used.

The dataset used is open source and is widely used.

Dataset available @ - https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)

Dataset characteristics

Datset contains 569 samples with 33 columns.
"""

# loading breast cancer dataset - Breast Cancer Wisconsin (Diagnostic) Data Set
# dataset source - https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
# dataframe creation from dataset
df = pd.read_csv("/content/drive/My Drive/Python Project data/BDMH_Project/wdbc.csv")
# drop last column as it is verbose
df.drop(df.columns[0], axis=1, inplace=True)
# drop 1st column as contains ID which is not of significance
df.drop(df.columns[-1], axis=1, inplace=True)
df.head()

# dataframe containing features of dataset
feature_df = df.iloc[:, 1:]
feature_df.head()

# analysis of features
feature_df.describe()

"""**Correlation analysis** between features. This is done to analyze which feature has higher correlation to which."""

# plotting correlation between the features for further analysis
correlation = feature_df.corr()
plt.rcParams.update({'font.size': 12})
# custom figure size for plot
plt.figure(figsize=(30,30))
# heatmap plot to represent correlation
sns.heatmap(correlation, annot=True, square=True, cmap='coolwarm')
plt.show()

"""From the analysis it was evident that **area_mean, perimeter_mean, perimeter_worst, area_worst** have a very high correlation hence they were removed from features.

After applying **PPS (Predictive Power Score)**
"""

new_cols = ['radius_mean','texture_mean','smoothness_mean','compactness_mean',
             'concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean',
            'radius_se','texture_se','perimeter_se','area_se','smoothness_se',
             'compactness_se','concavity_se','concave points_se','symmetry_se',
             'fractal_dimension_se','radius_worst','texture_worst',
             'smoothness_worst','compactness_worst','concavity_worst','concave points_worst',
             'symmetry_worst','fractal_dimension_worst']
pp_mean = pps.matrix(df[new_cols])
plt.figure(figsize=(30,30))
plt.rcParams.update({'font.size': 15})
sns.heatmap(pp_mean, annot=True, square=True, cmap='coolwarm')
plt.show()

"""# Creation of X and Y for training the machine learning models.

**Label encoder converts the 2 text classification label (M & B) into numeric 1 and 0.** M label is translated to 1 and B label is translated to 0.

Splitting data into **train and test with 75:25** ratio
"""

# dataframe X contains feature selected
X = df[new_cols]
# dataframe Y contains the corresponding labels
y = df.loc[:, 'diagnosis']
# label encoding is performed to convert Benign (0) and Malignant (1) labels in numeric series
# converting text labels to 2 class - 0 & 1
label_Y = LabelEncoder()
Y = label_Y.fit_transform(y)
# splitting data for training the classifiers and testing
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 500)

"""Using **MinMaxScaler** to normalize the train and test features"""

# scaling and normalization of X_train and X_test
# This estimator scales and translates each feature individually such that it is in the range 0-1
sc = MinMaxScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""Applying **PCA (Principal Component Analysis)** to reduce the dimensionality of the dataset resulting in increasing interpretability but at the same time minimizing information loss."""

pca = PCA()
X_train = pca.fit_transform(X_train) 
X_test = pca.transform(X_test)

"""lists used for plotting the accuracy comparison."""

# lists used for accuracy plots
accuracy_list = []
cv_accuracy_list = []
model_list = []

"""# **Random Forest Classifier**"""

# training Random Forest Classifier
classifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 500)
classifier_rf.fit(X_train, y_train)

"""**Predictions and Results** using Random Forest Classifer"""

# prediction of labels for the test data
y_pred = classifier_rf.predict(X_test)
# calculation of accuracy score based on predictions performed
# converting to Decimal as rounding with float is inaccurate
acc_score = round(Decimal(accuracy_score(y_test, y_pred) * 100), 2)
accuracy_list.append(acc_score)
model_list.append("RF")
print("Classification Report")
print(classification_report(y_test, y_pred))
print(f"Accuracy (RF Classifier with MinMax Scaling) = {acc_score}")

"""**Cross validation accuracy** for Random Forest Classifier with **5 folds of 80:20 splits of dataset** into train and test."""

# performing cross validation with 5 different splits
scores = cross_val_score(classifier_rf, X, y, cv=5)
# mean of cross val score (accuracy)
scores_mean = round(Decimal(scores.mean()*100), 2)
cv_accuracy_list.append(scores_mean)
print(f"Cross Validation Accuracy (RF Classifier with MinMax Scaling) = {scores_mean}")

"""# **K-Nearest Neighbor Classifier**"""

# training KNN classifier
classifier_knn = KNeighborsClassifier(n_neighbors=7, weights = 'distance', n_jobs = 4)
classifier_knn.fit(X_train, y_train)

"""**Prediction and Results** using K-Nearest Neighbor Classifier"""

# prediction of labels for the test data
y_pred = classifier_knn.predict(X_test)
# calculation of accuracy score based on predictions performed
acc_score = round(Decimal(accuracy_score(y_test, y_pred) * 100), 2)
accuracy_list.append(acc_score)
model_list.append("KNN")
print("Classification Report")
print(classification_report(y_test, y_pred))
print(f"Accuracy (KNN Classifier with MinMax Scaling) = {acc_score}")

"""**Cross Validation accuracy** for K-Nearest Neighbor Classifier with **5 folds of 80:20 splits of dataset** into train and test."""

# performing cross validation with 5 different splits
scores = cross_val_score(classifier_knn, X, y, cv=5)
# mean of cross val score (accuracy)
scores_mean = round(Decimal(scores.mean()*100), 2)
cv_accuracy_list.append(scores_mean)
print(f"Cross Validation Accuracy (KNN Classifier with MinMax Scaling) = {scores_mean}")

"""# **Logistic Regression Classifier**"""

# training logistic regression
classifier_lr = LogisticRegression(random_state=500)
classifier_lr.fit(X_train, y_train)

"""**Prediction and Results** using Logistic Regression Classifier"""

# prediction of labels for the test data
y_pred = classifier_lr.predict(X_test)
# calculation of accuracy score based on predictions performed
acc_score = round(Decimal(accuracy_score(y_test, y_pred) * 100), 2)
accuracy_list.append(acc_score)
model_list.append("LR")
print("Classification Report")
print(classification_report(y_test, y_pred))
print(f"Accuracy (LR Classifier with MinMax Scaling) = {acc_score}")

"""**Cross Validation accuracy** for Logistic Regression Classifier with **5 folds of 80:20 splits of dataset** into train and test."""

# performing cross validation with 5 different splits
scores = cross_val_score(classifier_lr, X, y, cv=5)
# mean of cross val score (accuracy)
scores_mean = round(Decimal(scores.mean()*100), 2)
cv_accuracy_list.append(scores_mean)
print(f"Cross Validation Accuracy (LR Classifier with MinMax Scaling) = {scores_mean}")

"""# **Support Vector Machine Classifier**"""

# parameter grid used for training and tuning SVM classifier
param_grid = {'C': [0.1, 1, 10, 40, 100, 200, 500, 1000],  
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 0.0008, 0.000009, 1e-010], 
              'kernel': ['rbf', 'linear']}
grid_svm = GridSearchCV(SVC(), param_grid, refit = True, verbose = 0)
grid_svm.fit(X_train, y_train)

"""Printing the best parameters found"""

# print best parameter after tuning 
print(grid_svm.best_params_) 
  
# print how the model looks after hyper-parameter tuning 
print(grid_svm.best_estimator_)

"""**Predictions and Results** using Support Vector Machine Classifier"""

# prediction of labels for the test data
grid_predictions = grid_svm.predict(X_test)
# calculation of accuracy score based on predictions performed
acc_score = round(Decimal(accuracy_score(y_test, grid_predictions) * 100), 2)
accuracy_list.append(acc_score)
model_list.append("SVM")
print("Classification Report")
print(classification_report(y_test, grid_predictions))
print(f"Accuracy (SVM Classifier with MinMax Scaling) = {acc_score}")

classifier_svm = SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,
    probability=False, random_state=None, shrinking=True, tol=0.001,
    verbose=False)
classifier_svm.fit(X_train, y_train)

"""**Cross Validation accuracy** of Support Vector Machine classifier with **5 folds of 80:20 splits of dataset** into train and test."""

# performing cross validation with 5 different splits
scores = cross_val_score(classifier_svm, X, y, cv=5)
# mean of cross val score (accuracy)
scores_mean = round(Decimal(scores.mean()*100), 2)
cv_accuracy_list.append(scores_mean)
print(f"Cross Validation Accuracy (SVM Classifier with MinMax Scaling) = {scores_mean}")

"""# **Multilayer Perceptron Classifier**"""

# training MLP Neural Network
classifier = Sequential()
# input layer with 64 neurons & shape (30, ) which is equal to number of features
classifier.add(Dense(64, activation='relu', input_shape=(26, )))
# hidden layer with 64 neurons
classifier.add(Dense(64, activation='relu'))
# output layer with a single neuron with activation function sigmoid
# reason for choosing sigmoid function is that it is most suitable for binary classification
# as it predicts the class with some probability value that is guaranteed to lie between 0 and 1
classifier.add(Dense(1, activation='sigmoid'))
# compiling the network
classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])
# classifier run for 50 iterations (epochs)
history = classifier.fit(X_train,y_train, batch_size=10, epochs=50)

"""Plotting **training accuracy and training loss curves**"""

plt.figure(figsize=(9, 9))
plt.plot(history.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train'], loc='upper left')
plt.show()
plt.figure(figsize=(9, 9))
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train'], loc='upper left')
plt.show()

"""**Predictions and Results** for Multilayer Perceptron"""

# predictions performed by MLP NN
predictions = classifier.predict(X_test)
# converting prediction probabilities into their appropriate labels
prediction_labels = []
# if predicted probability score is less than 0.50 it belongs to class 0 else class 1
for i in predictions:
  if i <= 0.50:
    class_label = 0
    prediction_labels.append(class_label)
  else:
    class_label = 1
    prediction_labels.append(class_label)
# calculation of accuracy score based on predictions performed
acc_score = round(Decimal(accuracy_score(y_test, prediction_labels) * 100), 2)
accuracy_list.append(acc_score)
model_list.append("MLP")
print("Classification Report")
print(classification_report(y_test, prediction_labels))
print(f"Accuracy (MLP Classifier with MinMax Scaling) = {acc_score}")

"""**Cross Validation accuracy** of Multilayer Perceptron Classifier with **5 folds of 80:20 splits of dataset** into train and test."""

# k fold cross validation with 5 folds
mlp_cv_acc = []
kf = StratifiedKFold(5, shuffle=True, random_state=500)
for train, test in kf.split(X, Y):
  X_train = X.iloc[train]
  y_train = Y[train]
  X_test = X.iloc[test]
  y_test = Y[test]
  classifier.fit(X_train,y_train, batch_size=10, epochs=50)
  # predictions performed by MLP NN
  predictions = classifier.predict(X_test)
  # converting prediction probabilities into their appropriate labels
  prediction_labels = []
  # if predicted probability score is less than 0.50 it belongs to class 0 else class 1
  for i in predictions:
    if i <= 0.50:
      class_label = 0
      prediction_labels.append(class_label)
    else:
      class_label = 1
      prediction_labels.append(class_label)
  # calculation of accuracy score based on predictions performed
  acc_score = round(accuracy_score(y_test, prediction_labels) * 100, 2)
  mlp_cv_acc.append(acc_score)
cv_accuracy_list.append(round(Decimal(mean(mlp_cv_acc)), 2))

df = pd.read_csv("/content/drive/My Drive/Python Project data/BDMH_Project/wdbc.csv")
df.drop(df.columns[-1], axis=1, inplace=True)
feature_df = df.iloc[:, 1:]
pred = pd.get_dummies(feature_df["diagnosis"],prefix='diag')
pred.drop(pred.columns[0], axis=1, inplace=True) ## M=1, B= 0
pred.columns = ['diagnosis']
feature_df = feature_df.iloc[:,1:]
feature_df['diagnosis'] = pred

"""Preparation of tabular data bunch for FastAI learning"""

sc = MinMaxScaler()
pca = PCA()
train = pd.DataFrame(sc.fit_transform(feature_df))
train = pd.DataFrame(pca.fit_transform(train))
procs = [FillMissing, Categorify, Normalize]
valid_idx = range(len(df)-142, len(df))
dep_var = 'diagnosis'
data = TabularDataBunch.from_df(path='.', df=df, dep_var=dep_var, valid_idx=valid_idx, procs=procs, cat_names=[])

data.show_batch(rows=10)

"""# **FastAI Classifier**"""

learned = tabular_learner(data, layers=[2000,500], emb_drop=0.1, metrics=accuracy)
learned.fit_one_cycle(10, max_lr=slice(1e-03))
learned.recorder.plot_metrics()
learned.recorder.plot()
learned.recorder.plot_losses() 
acc_score = round(Decimal(float(learned.recorder.metrics[-1][0])) * 100, 2)
accuracy_list.append(acc_score)
model_list.append("FastAI")
print(f"Accuracy (FastAI Classifier with MinMax Scaling) = {acc_score}")

"""**Cross Validation accuracy** of FastAI classifier with **5 folds of 80:20 splits of dataset** into train and test."""

sc = MinMaxScaler()
train = pd.DataFrame(sc.fit_transform(feature_df))
train = pd.DataFrame(pca.fit_transform(train))
procs = [FillMissing, Categorify, Normalize]
dep_var = 'diagnosis'
fastai_cv_acc = []
for i in range(5):
  df = shuffle(df)
  valid_idx = range(len(df)-114, len(df))
  data = TabularDataBunch.from_df(path='.', df=df, dep_var=dep_var, valid_idx=valid_idx, procs=procs, cat_names=[])
  learned = tabular_learner(data, layers=[2000,500], emb_drop=0.1, metrics=accuracy)
  learned.fit_one_cycle(10, max_lr=slice(1e-03))
  acc_score = round(float(learned.recorder.metrics[-1][0]) * 100, 2)
  fastai_cv_acc.append(acc_score)
cv_accuracy_list.append(round(Decimal(mean(fastai_cv_acc)), 2))

"""# **Model vs Accuracy comparison plot**"""

# comparison plot for all classifiers with their accuracy
plt.style.use('seaborn-poster')
fig = plt.figure(figsize=(10,10))
ax = fig.add_subplot()
plt.rcParams.update({'font.size': 22})
plt.title("Model Vs Accuracy Comparison")
plt.xlabel('Classifier')
plt.ylabel('Accuracy (%)')
plt.bar(model_list, accuracy_list, color='#95BFA5')
for i, j in enumerate(accuracy_list):
    ax.text(float(i)-0.35, float(j)+0.7, str(j), color='#275918')
plt.show()

"""# **Model vs Cross Validation Accuracy comparison plot**"""

# comparison plot for all classifiers with their cross validation accuracy
plt.style.use('seaborn-poster')
fig = plt.figure(figsize=(10,10))
ax = fig.add_subplot()
plt.rcParams.update({'font.size': 22})
plt.title("Model Vs Cross Validation Accuracy Comparison")
plt.xlabel('Classifier')
plt.ylabel('CV Accuracy (%)')
plt.bar(model_list, cv_accuracy_list, color='#95BFA5')
for i, j in enumerate(cv_accuracy_list):
    ax.text(float(i)-0.35, float(j)+0.7, str(j), color='#275918')
plt.show()